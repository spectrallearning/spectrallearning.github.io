<html>
  <head>
    <title>Schedule and abstracts</title>
    <link rel="stylesheet" href="style.css" />
  </head>

  <body>
    <div class="nav">
      <ul>
        <li><a href="index.html">Main</a></li>
        <li><a href="cfp.html">Call for papers</a></li>
        <li><a href="invited.html">Invited speakers</a></li>
        <li><a href="schedule.html">Schedule and abstracts</a></li>
        <li><a href="bib.html">Selected bibliography</a></li>
        <li><a href="past.html">Past workshops and tutorials</a></li>
        <li><a href="organization.html">Organization</a></li>
      </ul>
    </div>

    <div class="main">
      <h1>Schedule and abstracts</h1>

      <p>Date: Wednesday, June 25, 2014</p>

      <p>Location: Convention Hall No. 4E (Level 1 of the Beijing International Convention Center)</p>

      <table>
        <tr>
          <td>09:00-10:00</td>
          <td><a href="#invited-talk1">Invited talk: Elina Robeva</a></td>
        </tr>

        <tr>
          <td>10:00-10:20</td>
          <td><a href="#posters">Poster spotlights</a></td>
        </tr>

        <tr>
          <td>10:20-10:40</td>
          <td>Coffee break</td>
        </tr>

        <tr>
          <td>10:40-11:40</td>
          <td><a href="#invited-talk2">Invited talk: Prateek Jain</a></td>
        </tr>

        <tr>
          <td>11:40-11:50</td>
          <td><a href="#posters">Poster spotlights</a></td>
        </tr>

        <tr>
          <td>11:50-12:30</td>
          <td><a href="#posters">Poster session</a></td>
        </tr>

        <tr>
          <td>12:30-14:00</td>
          <td>Lunch</td>
        </tr>

        <tr>
          <td>14:00-15:00</td>
          <td><a href="#invited-talk3">Invited talk: Aravindan Vijayaraghavan</a></td>
        </tr>

        <tr>
          <td>15:00-15:40</td>
          <td><a href="#posters">Poster session</a> / coffee break</td>
        </tr>

        <tr>
          <td>15:40-16:40</td>
          <td><a href="#invited-talk4">Invited talk: Anima Anandkumar</a></td>
        </tr>

      </table>

      <h2>Invited talks</h2>

      <h3><a name="invited-talk1"></a>Elina Robeva (UC Berkeley):</h3>

      <p><i>Title</i>: Orthogonally Decomposable Tensors<br />

      <i>Abstract</i>: We describe tensor decomposition and the tensor power
      method from an algebraic perspective. Given an orthogonally decomposable
      tensor, we give a formula for all of its eigenvectors. Moreover, we give
      polynomial equations that cut out the space of orthogonally decomposable
      tensors.</p>

      <p><a href="OdecoPresentationICML.pdf">slides</a></p>

      <h3><a name="invited-talk2"></a>Prateek Jain (MSR India):</h3>

      <p><i>Title</i>: Provable Non-convex Projections for Low-rank Matrix
      Recovery<br />

      <i>Abstract</i>: Typical low-rank matrix recovery problems such as
      low-rank matrix completion, robust PCA etc can be solved using non-convex
      projections onto the set of low-rank matrices. However, providing
      theoretical guarantees for such methods is difficult due to the
      non-convexity in projections. In this talk, we will discuss two of our
      recent results that show that non-convex projections based methods can be
      used to solve two important problems in this area: a) low-rank matrix
      completion, b) robust PCA. For low-rank matrix completion, we provide
      first provable algorithm with time and storage complexity O(n) (assuming
      rank to be constant) while avoiding any dependence on the condition no.
      of the underlying matrix to be recovered. For robust PCA, we provide
      first provable algorithm with time complexity O(n^2 r) which matches the
      time complexity of normal SVD and is faster than the usual
      nuclear+L_1-regularization methods that incur O(n^3) time complexity.
      This talk is based on joint works with Animashree Anandkumar, U N
      Niranjan, Praneeth Netrapalli, and Sujay Sanghavi.</p>

      <p><a href="Prateek_Spectral_ICML14.pdf">slides</a></p>

      <h3><a name="invited-talk3"></a>Aravindan Vijayaraghavan (CMU):</h3>

      <p><i>Title</i>: Smoothed Analysis of Tensor Decompositions and Learning
      Overcomplete Latent Variable Models<br />

      <i>Abstract</i>: Low-rank tensor decompositions, the high dimensional
      analog of matrix decompositions, are a powerful tool that arise in
      machine learning, statistics and signal processing. However, tensors pose
      significant algorithmic challenges and tensors analogs of much of the
      matrix algebra toolkit are unlikely to exist because of hardness results.
      For instance, efficient tensor decompositions in the overcomplete case
      (where rank exceeds dimension) are particularly challenging.<br /><br />

      I will introduce a smoothed analysis model for studying tensor
      decompositions. Smoothed analysis gives an appealing way to analyze non
      worst-case instances: the assumption here is that the model is not
      adversarially chosen, formalized by a perturbation of model parameters.
      We give efficient algorithms for decomposing tensors, even in the highly
      overcomplete case (rank being polynomial in the dimension) using smoothed
      analysis. This gives new efficient algorithms for learning probabilistic
      models like mixtures of axis-aligned gaussians and multi-view models even
      in highly overcomplete settings.<br /><br />

      Based on joint work with Aditya Bhaskara, Moses Charikar and Ankur Moitra.</p>

      <h3><a name="invited-talk4"></a>Anima Anandkumar (UC Irvine):</h3>

      <p><i>Title</i>: Learning Overcomplete Latent Variable Models Through
      Tensor Decompositions.<br />

      <i>Abstract</i>: Tensor decomposition provide a guaranteed approach to
      unsupervised learning of latent variable models such as Gaussian
      mixtures, topic models and independent components. For these models,
      tensors computed from low order moments (up to fourth order) are
      decomposed to yield the model parameters. For overcomplete models, where
      the latent dimensionality exceeds the observed dimensionality,
      decomposing the tensor is challenging, since the tensor rank exceeds the
      observed dimensionality. In this talk, I will present recent results on
      guaranteed recovery of overcomplete tensors for generic parameters. This
      is joint work with Majid Janzamin and Rong Ge.</p>

      <h2>Posters</h2>

      <a name="posters"></a>

      <h3>Contributed posters</h3>

      <ul>
        <li>Chi Wang, Xueqing Liu, Yanglei Song, and Jiawei Han, <i>Scalable Exact Inference for Topic Model</i>.</li>

        <li>Uri Shalit and Gal Chechik, <i>Coordinate-descent for learning orthogonal matrices through Givens rotations</i>.</li>

        <li>Guillaume Rabusseau and Fran&ccedil;ois Denis, <i>Learning Negative Mixture Models by Tensor Decompositions</i>.</li>

        <li>Han Zhao and Pascal Poupart, <i>A Sober Look at Spectral Learning</i>.</li>

      </ul>

      <h3>Invited posters</h3>

      <ul>
        <li>Yuekai Sun, Stratis Ioannidis, and Andrea Montanari, <i>Learning Mixtures of Linear Classifiers</i>.</li>

        <li>Fran&ccedil;ois Denis, Mattias Gybels, and Amaury Habrard, <i>Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning</i>.</li>

        <li>Arun Tejasvi Chaganty and Percy Liang, <i>Estimating Latent-Variable Graphical Models using Moments and Likelihoods</i>.</li>

        <li>Alon Vinnikov and Shai Shalev-Shwartz, <i>K-means Recovers ICA Filters when Independent Components are Sparse</i>.</li>

        <li>Borja Balle, William Hamilton, and Joelle Pineau, <i>Methods of Moments for Learning Stochastic Languages: Unified Presentation and Empirical Comparison</i>.</li>

        <li>Ariadna Quattoni, Borja Balle, Xavier Carreras, and Amir Globerson, <i>Spectral Regularization for Max-Margin Sequence Tagging</i>.</li>
	<li>Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li
	Zhang, <i>Learning Polynomials with Neural Networks</i>.</li>
	<li>Hossein Azari Soufiani, David Parkes, and Lirong Xia,
	<i>Computing Parametric Ranking Models via
	Rank-Breaking</i>.</li>
	
      </ul>

    </div>
  </body>

</html>

